Data Types
==========

Terms
-----
The basic terms of the system are LF expressions. This one datatype
will encompass both types and objects in LF; so in certain contexts
will we interpret a term as a type vs an object but in others they are
treated the same.

type term = Var of var
  | App of term * term list
  | Lam of tyctx * term
  | Pi of tyctx * term
  | DB of int
  | Susp of term * int *int *env
  | Ptr of ptr


VAR:
We use Var to represent all the different types of variables we can
have (constants, eigenvariables, nominals, logic <- in unification
only) and we would have something like in Abella:

type id = string
type tag = Eigen | Constant | Logic | Nominal
type var = {
  name : id ;
  tag : tag ;
  ts : int ;
}

The difference with Abella is that our variables are not typed, so I
have not included a type here. See question (1) below though because
we may want to be able to identify a type for a variable (but not
require that one is given). These types could be used in the weak well
formedness check if given, for example.

LAM & PI:
Similarly, the tyctx of the Lam and Pi constructors would be similar
to in Abella, but we would want to use LF "type" expressions for the
type component.

type tyctx = (id * term) list

APP:
see question (2) about possibly modifying how we represent application
terms to include implicit argument information.

Terms will have to be objects that we can unify in unification, use in
LF typing judgments that we may attempt to derive (in LF), use to
instantiate variables in a formula/sequent, and check for weak well
formedness under a particular typing context. If we do not explicitly
carry the relevant LF types within a sequent then they will also be
used in determining typing such that a formula/sequent is weakly well
typed.

LF Object Declaration
---------------------
An object constant declaration is one of the kinds of things that make
up an LF signature. It would include the constant name, its type
(represented as a term), and what the implicit arguments are (if
any). We will also need fixity, associativity, and precedence for
printing and parsing.

name, type, fixity, associativity, precedence, # implicit arguments
and obj_decl = ObjDecl of (Symb.symbol * term * fixity * assoc * int *
int)

Regardless of the answer to question (2) we will definitely want
information about implicit arguments attached to the declarations for
use in reconstruction.

Object declarations will be used in parsing user input, performing
reconstruction, in checking derivability of judgments in LF, checking
weak well formedness, and in generating cases during case analysis.

LF Type Declaration
-------------------
A type declaration is one of the kinds of things that will be part of
the LF signature. It should track things like constant name, kind
(represented as a term), implicit args (if any), and a reference to
all the objects that construct terms of this type. Other relevant info
would be the fixity, associativity, and precedence to use in printing
and parsing.

name, kind, fixity, associativity, precedence, associated object constants, # implicit elements
type type_decl = TypeDecl of (Symb.symbol * term * fixity * assoc * int * Symb.symbol list ref * int)

Regardless of the answer to question (2) we will definitely want
information about implicit arguments attached to the declarations for
use in reconstruction.

The references to relevant objects would be helpful in performing case
analysis since it would immediately identify those object-level
constants in the signature that could possibly match the given type
(since we know head of a base type must be a constant).

Type declarations are used in parsing user input, performing
reconstruction on implicit LF, checking derivability of LF judgments,
checking weak well formedness of terms, and in case analysis.

LF Signature
------------
Reasoning must be done relative to a well formed LF signature which
declares some type & object level constants.

I think a similar representation to the logic programming
representation should work since we will want to access the
information in the signature in a similar way (used for coming up with
cases in case analysis and also used to check derivability of ground
judgments).
But in Abella, the LP module and signature are treated as lists. I
think this is because these become treated as a definition in the
reasoning system and so wouldn't be used directly. In our case we will
want to be accessing it directly during reasoning to check
derivability, perform case analysis, etc. so we would want a different
data type.

A signature would be made up of a lookup table for the type-level
constants and a lookup table for the object-level constants.

type signature = Signature of (Lfabsyn.typefam Symboltable.table * Lfabsyn.obj Symboltable.table)

The signature is needed to do reconstruction on terms, identify the
possible cases of case analysis, and to check if particular LF
judgments are derivable. It would also be useful in checking weak well
formedness by providing the kinds/types for constants.

Schema Definitions
------------------
TBD

Context Variables & Context Types
---------------------------------

Unification Problem
-------------------
The unification procedure will attempt to match two given terms, but
we want to allow non-llambda fragment so we should also keep a list of
current equations to solve. This way if one is stuck because it is not
llambda we can try solving the other equations before returning to
it. So a unification problem would be this list of pairs.

Sequent
-------
The main piece of data that is manipulated by the system is the
sequent since this is  what represents the proof state. This data
types is key to being able to design all the parts involved in
constructing proofs.

components of a sequent:
  eigenvariables
  context variables
  assumptions
  goal

reasoning in the system is also in the context of a particular LF
signature which declares the kinds/types for type/object level
constants as well as a collection of context schemas, so we will want
to make sure these are available in whatever context the manipulation
of sequents occurs in.

In Abella the sequents include a name, an index to the next subgoal,
and a "count". See question (3) about what this count represents.

Sequents will be manipulated using the proof rules for the logic. This
means that the current sequent will be changing a lot over time as the
proof is developed.

Prover State
------------
A state of the prover will need to know all of the possible context
needed to perform any of the commands/tactics. In Abella this
information is kept in prover.ml and isn't a datatype, but a
collection of references that are updated as needed.

  - the LF signature
  - context schema definitions
  - current sequent
  - list of remaining subgoals
  - available lemmas

The LF signature would remain fixed after being introduced.

Schema definitions will be added and then used in both checking that
context expressions satisfy the appropriate schema and also to
generate cases during case analysis.

The current sequent is updated by rule applications in constructing a
derivation.

The list of subgoals will track the other sequents that arise during
the course of a derivation. It would make sense for this to be a stack
structure so that all the cases of the most recent rule application
are considered first, then returning to the subgoals of a previous
state.

The available lemmas would be updated each time a proof is
successfully completed and then future derivations can refer to these
in the course of their construction.


Questions:
==========
1. to maintain weak well-formedness we need to type check instantiations
of terms (see proof argument). But I don't think we want to always
carry around the LF type assignments that demonstrate the weak
well-typing of a sequent. Is it efficient to determine the type to
check the term at when applying the tactic? [maybe; and could allow
user to specify a type to check the term at if generating is difficult
since checking if a given type works should be easy] Should we allow
any term instantiation even though it might break weak well
formedness? [I don't think so]

2. should the signature be where we get information about implicit
arguments, or should an application term itself identify implicit vs
explicit arguments? In the former case (like logic programming
implementation) must look up a constant in the signature each time we
want to know how many implicit arguments there are so we can elide
them. Since we only ever care about implicit arguments when printing
this is ok. In the later case we would include in an App term either
two term lists for the implicit and explicit arguments or an integer
identifying how many implicit arguments there are (implicit arguments
always appear first).

3. In Abella, sequents contain data that I can make sense of except
for this "count" value. What is it for? In the prover.ml file it is
only used in generating names for hypotheses? Is it a count of the
number of general (i.e. non special like IH) hypotheses?
